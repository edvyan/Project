{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, BertConfig, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "\n",
    "# Load the configuration of the pre-trained BERT model\n",
    "config = BertConfig.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "config.num_labels = 3  # Update the number of labels to 3 for 'O', 'B-ORG', 'I-ORG'\n",
    "\n",
    "model = BertForTokenClassification(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('../data/stock/stock.csv')\n",
    "df['sentence'] = \"Our analysis focuses on \" + df['Company Name'] + \".\"\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "sentences = [\"Our analysis focuses on \" + name + \".\" for name in df['Company Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(sentences):\n",
    "    labels_aligned = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        labels = ['O'] * len(tokens)  \n",
    "        start_index = 4  \n",
    "\n",
    "        if start_index < len(labels):\n",
    "            labels[start_index] = 'B-ORG'\n",
    "            for i in range(start_index + 1, len(tokens)):\n",
    "                if tokens[i].startswith('##') or tokens[i] in {',', '.'}:\n",
    "                    labels[i] = 'I-ORG'\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        labels_aligned.append(labels)\n",
    "\n",
    "    return labels_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "labels = prepare_labels(sentences)\n",
    "\n",
    "# Convert labels to IDs\n",
    "label_dict = {'O': 0, 'B-ORG': 1, 'I-ORG': 2}\n",
    "labels_ids = [[label_dict[label] for label in sent_labels] for sent_labels in labels]\n",
    "\n",
    "# Tokenize inputs and align labels with tokens\n",
    "encoding = tokenizer(sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_aligned = []\n",
    "for i, label in enumerate(labels_ids):\n",
    "    label_aligned = []\n",
    "    word_ids = encoding.word_ids(batch_index=i)\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None or word_idx != previous_word_idx:\n",
    "            label_aligned.append(label[word_idx] if word_idx is not None else -100)\n",
    "        else:\n",
    "            label_aligned.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    labels_aligned.append(label_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CompanyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_align_labels(df, tokenizer):\n",
    "    sentences = df['sentence'].tolist()\n",
    "    labels = prepare_labels(sentences, tokenizer)  \n",
    "    label_dict = {'O': 0, 'B-ORG': 1, 'I-ORG': 2}\n",
    "\n",
    "    tokenized_inputs = tokenizer(sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    labels_ids = [[label_dict[label] for label in sent_labels] for sent_labels in labels]\n",
    "\n",
    "    labels_aligned = []\n",
    "    for i, (label, encoding) in enumerate(zip(labels_ids, tokenized_inputs.encodings)):\n",
    "        word_ids = encoding.word_ids  #  word IDs \n",
    "        label_aligned = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx != previous_word_idx:\n",
    "                label_aligned.append(label[word_idx] if word_idx is not None else -100)\n",
    "            else:\n",
    "                label_aligned.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels_aligned.append(torch.tensor(label_aligned, dtype=torch.long).to(device))\n",
    "\n",
    "    input_ids = tokenized_inputs['input_ids'].to(device)\n",
    "    attention_mask = tokenized_inputs['attention_mask'].to(device)\n",
    "    return CompanyDataset(input_ids, attention_mask, labels_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CompanyDataset(encoding, labels_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16, \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',           \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = encode_and_align_labels(train_df, tokenizer)\n",
    "eval_sentences = encode_and_align_labels(val_df, tokenizer)\n",
    "\n",
    "# prepare labels and encode them\n",
    "eval_labels = prepare_labels(eval_sentences)\n",
    "eval_labels_ids = [[label_dict[label] for label in sent_labels] for sent_labels in eval_labels]\n",
    "\n",
    "# Encode the eval data\n",
    "eval_encoding = tokenizer(eval_sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Align labels for eval dataset\n",
    "eval_labels_aligned = []\n",
    "for i, label in enumerate(eval_labels_ids):\n",
    "    label_aligned = []\n",
    "    word_ids = eval_encoding.word_ids(batch_index=i)\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None or word_idx != previous_word_idx:\n",
    "            label_aligned.append(label[word_idx] if word_idx is not None else -100)\n",
    "        else:\n",
    "            label_aligned.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    eval_labels_aligned.append(label_aligned)\n",
    "\n",
    "# eval dataset\n",
    "eval_dataset = CompanyDataset(eval_encoding, eval_labels_aligned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b21f68fb8a2417485589d15a90708fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elena\\AppData\\Local\\Temp\\ipykernel_11792\\231676315.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7613, 'grad_norm': 13.175254821777344, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016042005b1447278abe1ca0d3ea0469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6071951389312744, 'eval_runtime': 4.8727, 'eval_samples_per_second': 0.616, 'eval_steps_per_second': 0.205, 'epoch': 1.0}\n",
      "{'loss': 0.7139, 'grad_norm': 8.446364402770996, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1955929b6d384f33ad91aa2e98bdfdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.502784252166748, 'eval_runtime': 5.3477, 'eval_samples_per_second': 0.561, 'eval_steps_per_second': 0.187, 'epoch': 2.0}\n",
      "{'loss': 0.5651, 'grad_norm': 5.8338775634765625, 'learning_rate': 3e-06, 'epoch': 2.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b16b6f708a048f2885afcecbdb0a273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6190041303634644, 'eval_runtime': 6.1238, 'eval_samples_per_second': 0.49, 'eval_steps_per_second': 0.163, 'epoch': 3.0}\n",
      "{'train_runtime': 255.2692, 'train_samples_per_second': 2.339, 'train_steps_per_second': 0.153, 'train_loss': 0.6130123688624456, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=0.6130123688624456, metrics={'train_runtime': 255.2692, 'train_samples_per_second': 2.339, 'train_steps_per_second': 0.153, 'total_flos': 24906482314614.0, 'train_loss': 0.6130123688624456, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
